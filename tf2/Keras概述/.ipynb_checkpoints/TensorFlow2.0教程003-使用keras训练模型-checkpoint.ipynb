{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow2教程-使用keras训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "本指南包含了TensorFlow 2.0中在以下两种情况下的训练，评估和预测（推理）模型：\n",
    "\n",
    "+ 使用内置的训练和评估API（例如model.fit()，model.evaluate()，model.predict()）。\n",
    "+ 使用eager execution 和GradientTape对象从头开始编写自定义循环。\n",
    "\n",
    "无论是使用内置循环还是编写自己的循环，模型和评估训练在每种Keras模型中严格按照相同的方式工作，无论是Sequential 模型, 函数式 API, 还是模型子类化。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 一般的模型构造、训练、测试流程\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用内置的训练和评估API对模型进行训练和验证。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型构造\n",
    "inputs = keras.Input(shape=(784,), name='mnist_input')\n",
    "h1 = layers.Dense(64, activation='relu')(inputs)\n",
    "h1 = layers.Dense(64, activation='relu')(h1)\n",
    "outputs = layers.Dense(10, activation='softmax')(h1)\n",
    "model = keras.Model(inputs, outputs)\n",
    "# keras.utils.plot_model(model, 'net001.png', show_shapes=True)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=[keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "端到端的模型训练。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3508 - sparse_categorical_accuracy: 0.9007 - val_loss: 0.2387 - val_sparse_categorical_accuracy: 0.9254\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1677 - sparse_categorical_accuracy: 0.9501 - val_loss: 0.1327 - val_sparse_categorical_accuracy: 0.9612\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1229 - sparse_categorical_accuracy: 0.9633 - val_loss: 0.1207 - val_sparse_categorical_accuracy: 0.9633\n",
      "history:\n",
      "{'loss': [0.3507663309574127, 0.16772893071174622, 0.1228933036327362], 'sparse_categorical_accuracy': [0.9007200002670288, 0.9501000046730042, 0.9632599949836731], 'val_loss': [0.23867085576057434, 0.13269120454788208, 0.12073920667171478], 'val_sparse_categorical_accuracy': [0.9254000186920166, 0.9611999988555908, 0.9632999897003174]}\n",
      "79/79 [==============================] - 0s 859us/step - loss: 0.1270 - sparse_categorical_accuracy: 0.9624\n",
      "evaluate:\n",
      "[0.12699058651924133, 0.9624000191688538]\n",
      "predict:\n",
      "[[4.3372296e-07 9.3322342e-08 1.0640792e-04 4.9457437e-04 1.4726362e-08\n",
      "  3.0706533e-07 5.5323615e-12 9.9935931e-01 4.2548677e-06 3.4671011e-05]\n",
      " [1.2598156e-08 6.5163831e-04 9.9910492e-01 2.1866495e-04 2.6674784e-11\n",
      "  1.7986398e-05 6.7330387e-07 8.1296031e-10 6.0268731e-06 2.5455263e-10]]\n"
     ]
    }
   ],
   "source": [
    "# 载入数据\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') /255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') /255\n",
    "\n",
    "# 保证还是float 32？ 否则后面会出现：TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type uint8 of argument 'x'.\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "# 取验证数据\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=3,\n",
    "         validation_data=(x_val, y_val))\n",
    "print('history:')\n",
    "print(history.history)\n",
    "\n",
    "result = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('evaluate:')\n",
    "print(result)\n",
    "pred = model.predict(x_test[:2])\n",
    "print('predict:')\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 自定义指标和损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 配置网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在对模型训练之前，我们需要指定损失函数，优化器以及可选的一些要监控的指标。我们将这些配置参数作为compile()方法的参数传递给模型，对模型进行配置。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=[keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow2提供许多内置的优化器，损失和指标 常见的内置参数如下：\n",
    "\n",
    "+ 优化器： - SGD()（有或没有动量） - RMSprop() - Adam() -等等。\n",
    "\n",
    "+ 损失： - MeanSquaredError() - KLDivergence() - CosineSimilarity() -等等。\n",
    "\n",
    "+ 指标： - AUC() - Precision() - Recall() -等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 自定义损失\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用Keras提供两种方式来提供自定义损失。\n",
    "\n",
    "+ 一、例创建一个接受输入y_true和的函数y_pred。\n",
    "+ 二、构建一个继承keras.losser.Loss的子类 下面示例显示了一个损失函数，该函数计算实际数据与预测之间的平均距离：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 1s 950us/step - loss: 4.3488\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 1s 921us/step - loss: 4.3488\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 1s 927us/step - loss: 4.3488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x65a032690>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_uncompiled_model():\n",
    "    inputs=keras.Input(shape=(784,),name='digits')\n",
    "    x = layers.Dense(64,activation='relu',name='dense_1')(inputs)\n",
    "    x = layers.Dense(64,activation='relu',name='dense_2')(x)\n",
    "    outputs=layers.Dense(10,activation='softmax',name='predictions')(x)\n",
    "    model=keras.Model(inputs=inputs,outputs=outputs)\n",
    "    return model\n",
    "model =get_uncompiled_model()\n",
    "\n",
    "def basic_loss_function(y_true,y_pred):\n",
    "    return tf.math.reduce_mean(y_true-y_pred)\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=basic_loss_function)\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要试下带参数的损失函数，可以子类化tf.keras.losses.Loss。并子类化以下方法：\n",
    "\n",
    "+ __init__(self) 接收相关参数，初始化loss之类。\n",
    "+ call(self, y_true, y_pred) 使用 y_true和y_pred，计算模型损失。\n",
    "\n",
    "下面例子，展示了WeightedCrossEntropy计算二分损失的损失函数，某个类或整个函数的损失可以通过标量修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 1s 999us/step - loss: 9.5182\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 1s 966us/step - loss: 9.5170\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 9.5171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x65cf09ed0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WeightBinaryCrossEntropy(keras.losses.Loss):\n",
    "    def __init__(self,pos_weight,weight,from_logits=False,\n",
    "                reduction=keras.losses.Reduction.AUTO,\n",
    "                name='weight_binary_crossentropy'):\n",
    "        \"\"\"\n",
    "        pos_weight: 正类标签权重\n",
    "        weight: 整体损失权重\n",
    "        from_logits: 是否使用logits来计算loss，（或使用probability）\n",
    "        reduction: reduction类型\n",
    "        name: 名字\n",
    "        \"\"\"\n",
    "        super (WeightBinaryCrossEntropy,self).__init__(reduction=reduction,name=name)\n",
    "        self.pos_weight=pos_weight\n",
    "        self.weight=weight\n",
    "        self.from_logits=from_logits\n",
    "    def call(self,y_true,y_pred):\n",
    "        if not self.from_logits:\n",
    "            x_1 =y_true*self.pos_weight* -tf.math.log(y_pred+1e-6)\n",
    "            \n",
    "            x_2 = (1-y_true)* -tf.math.log(1-y_pred+1e-6)\n",
    "            \n",
    "            return tf.add(x_1,x_2)*self.weight\n",
    "        return tf.nn.weighted_cross_entropy_with_logits(y_true,y_pred,self.pos_weight)*self.weight\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss = WeightBinaryCrossEntropy(0.5,2))\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 自定义指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义指标只需继承Metric类， 并重写以下函数：\n",
    "\n",
    "+ __init__(self)，初始化。\n",
    "\n",
    "+ update_state(self，y_true，y_pred，sample_weight = None)，它使用目标y_true和模型预测y_pred来更新状态变量。\n",
    "\n",
    "+ result(self)，它使用状态变量来计算最终结果。\n",
    "\n",
    "+ reset_states(self)，重新初始化度量的状态。\n",
    "\n",
    "状态更新和结果计算保持分开（分别在update_state()和result()中），因为在某些情况下，结果计算可能非常昂贵，并且只能定期进行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4803 - binary_true_postives: 8350.0000\n",
      "Epoch 2/3\n",
      "142/782 [====>.........................] - ETA: 0s - loss: 0.1943 - binary_true_postives: 1494.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinyang/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:2034: UserWarning: Metric CatgoricalTruePostives implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1727 - binary_true_postives: 7943.0000\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1288 - binary_true_postives: 7907.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x65cf5c8d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面是一个简单的示例，显示如何实现CatgoricalTruePositives指标，该指标计算正确分类为属于给定类的样本数量\n",
    "\n",
    "class CatgoricalTruePostives(keras.metrics.Metric):\n",
    "    def __init__(self,name='binary_true_postives',**kwargs):\n",
    "        super(CatgoricalTruePostives,self).__init__(name=name,**kwargs)\n",
    "        \n",
    "         # 会更新的类变量\n",
    "        self.true_postives=self.add_weight(name='tp',initializer='zeros')\n",
    "    def update_state(self,y_true,y_pred,sample_weight=None):\n",
    "        # 获取结果id\n",
    "        y_pred=tf.argmax(y_pred)\n",
    "        # 正确的结果\n",
    "        y_true=tf.equal(tf.cast(y_pred,tf.int32),tf.cast(y_true,tf.int32))\n",
    "        y_true=tf.cast(y_true,tf.float32)\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            # 对正确结果加权重\n",
    "            sample_weight=tf.cast(sample_weight,tf.float32)\n",
    "            y_true = tf.multiply(sample_weight,y_true)\n",
    "        # 修改正确样本总量\n",
    "        return self.true_postives.assign_add(tf.reduce_sum(y_true)) # assign_add：将值加到self.true_postives\n",
    "    def result(self):\n",
    "        # 返回相应tensor\n",
    "        return tf.identity(self.true_postives) # tf.identity：读取 self.true_postives，和assign_add 配对使用\n",
    "    def reset_states(self):\n",
    "        # 重置为0\n",
    "        self.true_postives.assign(0.)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=[CatgoricalTruePostives()])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "         batch_size=64, epochs=3)\n",
    "            \n",
    "                \n",
    "#自定义指标类里面一定要有result，update_state,reset_states 三个方法        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用自定义层的方式获取相关指标\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2986 - sparse_categorical_accuracy: 0.9136 - std_of_activation: 1.0189\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.1405 - sparse_categorical_accuracy: 0.9578 - std_of_activation: 1.0927\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.1051 - sparse_categorical_accuracy: 0.9688 - std_of_activation: 1.1693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x661553990>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以以定义网络层的方式添加要统计的metric\n",
    "class MetricLoggingLayer(layers.Layer):\n",
    "    def call(self,inputs):\n",
    "        # 该层的作用就是添加指标\n",
    "        self.add_metric(keras.backend.std(inputs),\n",
    "                       name='std_of_activation',\n",
    "                       aggregation='mean')\n",
    "        # 直接把输入进行输出\n",
    "        return inputs\n",
    "inputs = keras.Input(shape=(784,),name='mnist_input')\n",
    "h1 = layers.Dense(64,activation='relu')(inputs)\n",
    "\n",
    "# 直接套在对应的网络层中\n",
    "h1 = MetricLoggingLayer()(h1)\n",
    "h1 = layers.Dense(64,activation='relu')(h1)\n",
    "outputs = layers.Dense(10,activation='softmax')(h1)\n",
    "model = keras.Model(inputs,outputs)\n",
    "keras.utils.plot_model(model, 'net001.png', show_shapes=True)\n",
    "# 配置并训练网络\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "             loss= keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(x_train,y_train,batch_size=32,epochs=3)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以在构建好模型后直接使用，model.add_loss和model.add_metric添加损失和指标。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3459 - sparse_categorical_accuracy: 0.1132 - std_of_activation: 0.0010\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3012 - sparse_categorical_accuracy: 0.1136 - std_of_activation: 7.5434e-07\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3013 - sparse_categorical_accuracy: 0.1136 - std_of_activation: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3012 - sparse_categorical_accuracy: 0.1136 - std_of_activation: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3012 - sparse_categorical_accuracy: 0.1136 - std_of_activation: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3012 - sparse_categorical_accuracy: 0.1136 - std_of_activation: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3012 - sparse_categorical_accuracy: 0.1136 - std_of_activation: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3012 - sparse_categorical_accuracy: 0.1136 - std_of_activation: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3012 - sparse_categorical_accuracy: 0.1136 - std_of_activation: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 2.3012 - sparse_categorical_accuracy: 0.1136 - std_of_activation: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6614fc250>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=keras.Input(shape=(784,),name='mnist_input')\n",
    "h1 =layers.Dense(64,activation='relu')(inputs)\n",
    "h2 = layers.Dense(10,activation='relu')(h1)\n",
    "outputs = layers.Dense(10,activation='softmax')(h2)\n",
    "model = keras.Model(inputs,outputs)\n",
    "# 直接把计算loss或metric用到的输入(h1)带人\n",
    "model.add_metric(keras.backend.std(h1),\n",
    "                name='std_of_activation',\n",
    "                aggregation='mean')\n",
    "model.add_loss(tf.reduce_sum(h1)*0.1)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(x_train,y_train,batch_size=32,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理使用validation_data传入测试数据，还可以使用validation_split划分验证数据\n",
    "\n",
    "ps:validation_split只能在用numpy数据训练的情况下使用\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 使用tf.data构造数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到现在我们已经了解了如何使用使用numpy作为输入数据进行训练和验证。下面，我们将介绍如何使用tf.data作为输入数据进行。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_compiled_model():\n",
    "    inputs=keras.Input(shape=(784,),name='mnist_input')\n",
    "    h1=layers.Dense(64,activation='relu')(inputs)\n",
    "    h2=layers.Dense(64,activation='relu')(h1)\n",
    "    outputs=layers.Dense(10,activation='softmax')(h2)\n",
    "    model=keras.Model(inputs,outputs)\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "                 loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return model\n",
    "model=get_compiled_model()\n",
    "# 构建dataset实例\n",
    "train_dataset=tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "# 打乱\n",
    "train_dataset=train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "# 获得验证数据\n",
    "val_dataset=tf.data.Dataset.from_tensor_slices((x_val,y_val))\n",
    "val_dataset=val_dataset.batch(64)\n",
    "\n",
    "# model.fit(train_dataset, epochs=3)\n",
    "# steps_per_epoch 每个epoch只训练几步\n",
    "# validation_steps 每次验证，验证几步\n",
    "\n",
    "model.fit(train_dataset,epochs=3,steps_per_epoch=100,\n",
    "         validation_data=val_dataset,validation_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果只想对该数据集中的特定批次进行训练，则可以传递steps_per_epoch参数，即批训练多少步。同样我们可以使用train_dataset.take(), 来获取每批次中训练的数据，其和steps_per_epoc等价。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "model.fit(train_dataset.take(100), epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 模型测试\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "print('\\n# Evaluate')\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他格式输入数据支持\n",
    "\n",
    "除了numpy数值和TensorFlow Dataset,还可以用Pandas和python迭代器作为数据输入。 通常小数据推荐使用numpy， 大数据推荐使用TensorFlow Dataset。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 2s 2ms/step - loss: 2.3011 - sparse_categorical_accuracy: 0.1141 - std_of_activation: 0.0000e+00 - val_loss: 2.3015 - val_sparse_categorical_accuracy: 0.1115 - val_std_of_activation: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x661bd3f10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 使用tf.data构造数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到现在我们已经了解了如何使用使用numpy作为输入数据进行训练和验证。下面，我们将介绍如何使用tf.data作为输入数据进行。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "100/100 [==============================] - 1s 2ms/step - loss: 0.7900 - sparse_categorical_accuracy: 0.7877 - val_loss: 0.4173 - val_sparse_categorical_accuracy: 0.8646\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3549 - sparse_categorical_accuracy: 0.9002 - val_loss: 0.3072 - val_sparse_categorical_accuracy: 0.8802\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.3144 - sparse_categorical_accuracy: 0.9070 - val_loss: 0.2344 - val_sparse_categorical_accuracy: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x65d82db50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_compiled_model():\n",
    "    inputs=keras.Input(shape=(784,),name='mnist_input')\n",
    "    h1=layers.Dense(64,activation='relu')(inputs)\n",
    "    h2=layers.Dense(64,activation='relu')(h1)\n",
    "    outputs=layers.Dense(10,activation='softmax')(h2)\n",
    "    model=keras.Model(inputs,outputs)\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "                 loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return model\n",
    "model=get_compiled_model()\n",
    "# 构建dataset实例\n",
    "train_dataset=tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "# 打乱\n",
    "train_dataset=train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "# 获得验证数据\n",
    "val_dataset=tf.data.Dataset.from_tensor_slices((x_val,y_val))\n",
    "val_dataset=val_dataset.batch(64)\n",
    "\n",
    "# model.fit(train_dataset, epochs=3)\n",
    "# steps_per_epoch 每个epoch只训练几步\n",
    "# validation_steps 每次验证，验证几步\n",
    "\n",
    "model.fit(train_dataset,epochs=3,steps_per_epoch=100,\n",
    "         validation_data=val_dataset,validation_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果只想对该数据集中的特定批次进行训练，则可以传递steps_per_epoch参数，即批训练多少步。同样我们可以使用train_dataset.take(), 来获取每批次中训练的数据，其和steps_per_epoc等价。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.8406 - sparse_categorical_accuracy: 0.7883\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3347 - sparse_categorical_accuracy: 0.9052\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2508 - sparse_categorical_accuracy: 0.9302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6614d62d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "model.fit(train_dataset.take(100), epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate\n",
      "157/157 [==============================] - 0s 920us/step - loss: 0.2908 - sparse_categorical_accuracy: 0.9150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2908131778240204, 0.9150000214576721]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型测试\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "print('\\n# Evaluate')\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他格式输入数据支持\n",
    "\n",
    "除了numpy数值和TensorFlow Dataset,还可以用Pandas和python迭代器作为数据输入。 通常小数据推荐使用numpy， 大数据推荐使用TensorFlow Dataset。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 样本权重和类权重\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在模型训练时可以，可以人工设定样本权重和类权重。\n",
    "\n",
    "“样本权重”数组是一个数字数组，用于指定批处理中每个样本在计算总损失时应具有多少权重。 它通常用于不平衡的分类问题（这个想法是为了给予很少见的类更多的权重）。 当使用的权重是1和0时，该数组可以用作损失函数的掩码（完全丢弃某些样本对总损失的贡献）。\n",
    "\n",
    "“类权重”dict是同一概念的更具体的实例：它将类索引映射到应该用于属于该类的样本的样本权重。 例如，如果类“0”比数据中的类“1”少两倍，则可以使用class_weight = {0：1.，1：0.5}。\n",
    "\n",
    "添加方法：\n",
    "\n",
    "+ 使用Numpy数据时： 通过sample_weight和class_weight参数传递。\n",
    "+ 使用Dataset数据时： 通过使数据集返回(input_batch, target_batch, sample_weight_batch)。\n",
    "下面是一个Numpy数据中加大第5类的权重的例子。\n",
    "\n",
    "下面是一个Numpy数据中加大第5类的权重的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加第5类的权重\n",
    "import numpy as np\n",
    "# 类权重\n",
    "model = get_compiled_model()\n",
    "class_weight={i:1.0 for i in range(10)}\n",
    "# 第5类的权重为2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
